{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=5.76s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from model.clip_base_sd import CLIP_SD\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "test_data_dir='/data/public/coco2014/val2014'\n",
    "test_list='/data/public/coco2014/annotations/instances_val2014.json'\n",
    "with open('./data/coco/category.json', 'r') as load_category:\n",
    "    category_map = json.load(load_category)\n",
    "coco = COCO(test_list)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "scale_size = 512\n",
    "crop_size = 448\n",
    "test_data_transform = transforms.Compose([transforms.Resize((scale_size, scale_size)),\n",
    "                                             transforms.CenterCrop(crop_size),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             normalize])\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--backbone_name',default='RN101')\n",
    "parser.add_argument('--crop_size',default=448)\n",
    "args = parser.parse_args([])\n",
    "graph_file='./data/coco/prob_train.npy'\n",
    "word_file='./data/coco/vectors.npy'\n",
    "with open('./data/coco/category_name.json', 'r') as load_category:\n",
    "        category_map = json.load(load_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "model = CLIP_SD(args=args,\n",
    "                classnames=category_map,\n",
    "                image_feature_dim=2048,\n",
    "                num_classes=80,\n",
    "                word_feature_dim=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('exp/checkpoint/SD-CLIP_BASE-COCO-exp3.1-lr3e-4-bs16/Checkpoint_Best.pth', map_location=torch.device('cpu'))\n",
    "best_prec = checkpoint['best_mAP']\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(coco.imgs.keys())\n",
    "img_id = ids[0]\n",
    "path = coco.loadImgs(img_id)[0]['file_name']\n",
    "input = Image.open(os.path.join(test_data_dir, path)).convert('RGB')\n",
    "input = test_data_transform(input).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 3 is out of bounds for array of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m cam \u001b[39m=\u001b[39m GradCAM(model\u001b[39m=\u001b[39mmodel, target_layers\u001b[39m=\u001b[39mtarget_layers, use_cuda\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m targets \u001b[39m=\u001b[39m [ClassifierOutputTarget(\u001b[39m0\u001b[39m)]\n\u001b[0;32m----> 4\u001b[0m grayscale_cam \u001b[39m=\u001b[39m cam(input_tensor\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m,targets\u001b[39m=\u001b[39;49mtargets)\n",
      "File \u001b[0;32m~/.conda/envs/osvr/lib/python3.8/site-packages/pytorch_grad_cam/base_cam.py:190\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m aug_smooth \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_augmentation_smoothing(\n\u001b[1;32m    188\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 190\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_tensor,\n\u001b[1;32m    191\u001b[0m                     targets, eigen_smooth)\n",
      "File \u001b[0;32m~/.conda/envs/osvr/lib/python3.8/site-packages/pytorch_grad_cam/base_cam.py:97\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     86\u001b[0m     loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m \u001b[39m# In most of the saliency attribution papers, the saliency is\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m# computed with a single target layer.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m# Commonly it is the last convolutional layer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39m# use all conv layers for example, all Batchnorm layers,\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m# or something else.\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m cam_per_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_cam_per_layer(input_tensor,\n\u001b[1;32m     98\u001b[0m                                            targets,\n\u001b[1;32m     99\u001b[0m                                            eigen_smooth)\n\u001b[1;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate_multi_layers(cam_per_layer)\n",
      "File \u001b[0;32m~/.conda/envs/osvr/lib/python3.8/site-packages/pytorch_grad_cam/base_cam.py:129\u001b[0m, in \u001b[0;36mBaseCAM.compute_cam_per_layer\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(grads_list):\n\u001b[1;32m    127\u001b[0m     layer_grads \u001b[39m=\u001b[39m grads_list[i]\n\u001b[0;32m--> 129\u001b[0m cam \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_cam_image(input_tensor,\n\u001b[1;32m    130\u001b[0m                          target_layer,\n\u001b[1;32m    131\u001b[0m                          targets,\n\u001b[1;32m    132\u001b[0m                          layer_activations,\n\u001b[1;32m    133\u001b[0m                          layer_grads,\n\u001b[1;32m    134\u001b[0m                          eigen_smooth)\n\u001b[1;32m    135\u001b[0m cam \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(cam, \u001b[39m0\u001b[39m)\n\u001b[1;32m    136\u001b[0m scaled \u001b[39m=\u001b[39m scale_cam_image(cam, target_size)\n",
      "File \u001b[0;32m~/.conda/envs/osvr/lib/python3.8/site-packages/pytorch_grad_cam/base_cam.py:50\u001b[0m, in \u001b[0;36mBaseCAM.get_cam_image\u001b[0;34m(self, input_tensor, target_layer, targets, activations, grads, eigen_smooth)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cam_image\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     43\u001b[0m                   input_tensor: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m     44\u001b[0m                   target_layer: torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m                   grads: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m     48\u001b[0m                   eigen_smooth: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m---> 50\u001b[0m     weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_cam_weights(input_tensor,\n\u001b[1;32m     51\u001b[0m                                    target_layer,\n\u001b[1;32m     52\u001b[0m                                    targets,\n\u001b[1;32m     53\u001b[0m                                    activations,\n\u001b[1;32m     54\u001b[0m                                    grads)\n\u001b[1;32m     55\u001b[0m     weighted_activations \u001b[39m=\u001b[39m weights[:, :, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m activations\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m eigen_smooth:\n",
      "File \u001b[0;32m~/.conda/envs/osvr/lib/python3.8/site-packages/pytorch_grad_cam/grad_cam.py:22\u001b[0m, in \u001b[0;36mGradCAM.get_cam_weights\u001b[0;34m(self, input_tensor, target_layer, target_category, activations, grads)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_cam_weights\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     17\u001b[0m                     input_tensor,\n\u001b[1;32m     18\u001b[0m                     target_layer,\n\u001b[1;32m     19\u001b[0m                     target_category,\n\u001b[1;32m     20\u001b[0m                     activations,\n\u001b[1;32m     21\u001b[0m                     grads):\n\u001b[0;32m---> 22\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mmean(grads, axis\u001b[39m=\u001b[39;49m(\u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m))\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/osvr/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3432\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3429\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3430\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3432\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_mean(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3433\u001b[0m                       out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/osvr/lib/python3.8/site-packages/numpy/core/_methods.py:168\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    164\u001b[0m arr \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[1;32m    166\u001b[0m is_float16_result \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m rcount \u001b[39m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[39m=\u001b[39;49mkeepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m rcount \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m where \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m umr_any(rcount \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    170\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mMean of empty slice.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mRuntimeWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/osvr/lib/python3.8/site-packages/numpy/core/_methods.py:76\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[1;32m     74\u001b[0m     items \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     75\u001b[0m     \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m axis:\n\u001b[0;32m---> 76\u001b[0m         items \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mshape[mu\u001b[39m.\u001b[39;49mnormalize_axis_index(ax, arr\u001b[39m.\u001b[39;49mndim)]\n\u001b[1;32m     77\u001b[0m     items \u001b[39m=\u001b[39m nt\u001b[39m.\u001b[39mintp(items)\n\u001b[1;32m     78\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     \u001b[39m# TODO: Optimize case when `where` is broadcast along a non-reduction\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[39m# axis and full sum is more excessive than needed.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m     \u001b[39m# guarded to protect circular imports\u001b[39;00m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 3 is out of bounds for array of dimension 3"
     ]
    }
   ],
   "source": [
    "target_layers = [model.clip_model.visual.layer4[-1]]\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\n",
    "targets = [ClassifierOutputTarget(0)]\n",
    "grayscale_cam = cam(input_tensor=input,targets=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67d341204c69a3bca71f70e3af6b31dd6f2104d6cd31e2fab53818f8bb9c03b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
