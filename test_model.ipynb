{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.clip_base_sd import CLIP_SD\n",
    "import argparse\n",
    "import json\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--backbone_name',default='RN101')\n",
    "parser.add_argument('--crop_size',default=448)\n",
    "args = parser.parse_args([])\n",
    "graph_file='./data/coco/prob_train.npy'\n",
    "word_file='./data/coco/vectors.npy'\n",
    "with open('./data/coco/category_name.json', 'r') as load_category:\n",
    "        category_map = json.load(load_category)\n",
    "model1 = CLIP_SD(args=args,\n",
    "                classname=category_map,\n",
    "                image_feature_dim=2048,\n",
    "                num_classes=80,\n",
    "                word_feature_dim=512,\n",
    ")\n",
    "for p in model1.parameters():\n",
    "        p.requires_grad = False\n",
    "for p in model1.word_semantic.parameters():\n",
    "        p.requires_grad = True\n",
    "for p in model1.classifiers.parameters():\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.coop_sd import COOP_SD\n",
    "import argparse\n",
    "import json\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--backbone_name',default='RN101')\n",
    "parser.add_argument('--crop_size',default=448)\n",
    "parser.add_argument('--n_ctx', default=16, type=int,\n",
    "                    help='nums of context')\n",
    "parser.add_argument('--ctx_init', action='store_true', default=False,\n",
    "                    help='init context')\n",
    "parser.add_argument('--csc', action='store_true', default=False,\n",
    "                    help='class special context')\n",
    "parser.add_argument('--class_token_position', default=\"end\",\n",
    "                    help='position of context')\n",
    "args = parser.parse_args([])\n",
    "graph_file='./data/coco/prob_train.npy'\n",
    "word_file='./data/coco/vectors.npy'\n",
    "with open('./data/coco/category_name.json', 'r') as load_category:\n",
    "        category_map = json.load(load_category)\n",
    "model2 = COOP_SD(args=args,\n",
    "                classnames=category_map,\n",
    "                image_feature_dim=2048,\n",
    "                num_classes=80,\n",
    "                word_feature_dim=512,\n",
    "                )\n",
    "for p in model2.parameters():\n",
    "        p.requires_grad = False\n",
    "for p in model2.word_semantic.parameters():\n",
    "        p.requires_grad = True\n",
    "for p in model2.classifiers.parameters():\n",
    "        p.requires_grad = True\n",
    "for p in model2.prompt_learner.parameters():\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_number(model):\n",
    "    total_num = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Total': 123564978, 'Trainable': 3835985}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_parameter_number(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Total': 123532210, 'Trainable': 3844177}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_parameter_number(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'COOP_SD' object has no attribute 'clip_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model2\u001b[39m.\u001b[39;49mclip_model\n",
      "File \u001b[0;32m~/.conda/envs/osvr/lib/python3.8/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'COOP_SD' object has no attribute 'clip_model'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones([1,3,448,448])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = model1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = model2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osvr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67d341204c69a3bca71f70e3af6b31dd6f2104d6cd31e2fab53818f8bb9c03b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
